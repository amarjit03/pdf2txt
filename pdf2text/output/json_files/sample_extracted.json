{
  "extraction_metadata": {
    "source_file": "sample.pdf",
    "processed_date": "2025-06-01T22:35:53Z",
    "agent_version": "1.0.0",
    "output_format": "json"
  },
  "document_info": {
    "pdf_type": "text_based",
    "page_count": 14,
    "file_size_mb": 14.52841567993164,
    "complexity": "moderate",
    "has_tables": false,
    "has_images": true,
    "has_forms": true
  },
  "extraction_details": {
    "method_used": "pymupdf_structured",
    "processing_time_seconds": 0.6201441287994385,
    "confidence_score": 1.0,
    "total_characters": 48813,
    "chunks_processed": 3
  },
  "content": {
    "extracted_text": "DeepFake MNIST+: A DeepFake Facial Animation Dataset\n\nJiajun Huang\n\nThe University of Sydney\n\nXueyu Wang\n\nThe University of Sydney\n\nBo Du\n\nWuhan University\n\nPei Du\n\nAntGroup\n\nChang Xu\n\nThe University of Sydney\n\nAbstract\n\nThe DeepFakes, which are the facial manipulation tech-\nniques, is the emerging threat to digital society. Various\nDeepFake detection methods and datasets are proposed for\ndetecting such data, especially for face-swapping. How-\never, recent researches less consider facial animation, which\nis also important in the DeepFake attack side. It tries to\nanimate a face image with actions provided by a driving\nvideo, which also leads to a concern about the security of\nrecent payment systems that reply on liveness detection to\nauthenticate real users via recognising a sequence of user\nfacial actions. However, our experiments show that the ex-\nisted datasets are not sufficient to develop reliable detection\nmethods. While the current liveness detector cannot defend\nsuch videos as the attack. As a response, we propose a new\nhuman face animation dataset, called DeepFake MNIST+ 1 ,\ngenerated by a SOTA image animation generator. It includes\n10,000 facial animation videos in ten different actions, which\ncan spoof the recent liveness detectors. A baseline detec-\ntion method and a comprehensive analysis of the method\nis also included in this paper. In addition, we analyze the\nproposed dataset’s properties and reveal the difficulty and\nimportance of detecting animation datasets under different\ntypes of motion and compression quality.\n\n1. Introduction\n\nDeepFake 2 has become a critical topic for our digital\nsociety. With DeepFake, we can now easily change the\nidentity or expression of the face in an image or a piece\nof video with another person’s identity or expression. The\nmainstream DeepFake techniques are based on Deep Neural\nNetworks (DNNs), especially Generative Adversarial Net-\nworks (GANs) [ 17 ], to produce visually plausible images or\nvideos which are hard to be discriminated by humans. There\nis growing concern about DeepFake, as malicious people\ncould use the techniques to palm off the victims and illude\npresence and activities, even if they never did before.\nA number of DeepFake methods have been developed\n\n1 https://github.com/huangjiadidi/DeepFakeMnist\n2 DeepFake not only indicates the facial modification methods but also\nis the name of one algorithm called deepfake [ 5 ].\n\nto manipulate the attributes of human face in images or\nvideos. For example, the swapping methods [ 6 , 5 , 4 , 8 , 3 , 25 ]\nmostly focus on the identity of the face and try to replace\nthe face in one image/video with the face from others. The\ndeepfake method [ 5 ], a famous swapping algorithm, trains\nidentity-dependent two auto-encoders to swap the faces of\ntwo identities. Besides face identity, many other face at-\ntributes have been studied in the literature. For example,\n[ 34 , 33 ] modified the expression in one face image/video,\nand facial image animation [ 11 , 37 , 31 , 32 , 12 ], as a com-\npose of expression manipulation, is increasingly being im-\nportant within the DeepFakes. Given a face source image\nand the driving video, DeepFake can now generate a new\nvideo where the source face performs the same action as\nthe driving video. For instance, Siarohin et al. [ 31 ] use an\nencoder to capture optical flow information from the videos,\nembedding the information with source images, and generate\nvideos. Zakharov et al. [ 39 ] pass the identity embedding to\nthe image generator to produce manipulated face with the\ngiven landmark. Burkov [ 12 ] extracts the identity and pose\ninformation separately and generate videos with embedding.\n\nGiven the growing anxiety on the high-quality generation\nby DeepFake and its potential negative social impacts, it\nbecomes especially urgent to study the defense techniques\nagainst DeepFake. Recently a few datasets have been created\nfor the study of DeepFake detection methods. UADFV [ 38 ]\nand Celeb-DF [ 27 ] collect youtube videos to generate face-\nswapping videos, and DFDC [ 16 ] captures 48,190 videos\nwith paid actors and generates a large scale swapping dataset\nwith over 104,500 videos.\nBy analyzing these datasets,\nRossler et al. [ 30 ] and Seferbekov [ 7 ] suggested the impor-\ntance of CNN architectures on detecting the swapped faces.\nLi et al. [ 26 ] further found the manipulate boundary between\nthe face and head can be an effective clue for the detection.\nHowever, all these works are mostly about detecting the face\nidentity change, which is a limitation of existing deepfake\ndatasets. There is rare dataset or deepfake detection work\nabout facial animation, though it occupies a significant part\nin DeepFake attack side.\n\nRecently, facial animation by DeepFake has been de-\nployed on mobile devices, producing a large number of fake\n\nvideos and broadcasts through the Internet [ 1 ]. These fake\nvideos thus challenge the security of many intelligent sys-\ntems in our daily life. For example, the face recognition\nbased payment systems usually rely on liveness detection to\nverify whether users are the real people by requiring them\nto do a sequence of specific actions in videos. However, our\nexperiments show that DeepFake detectors trained on exist-\ning deepfake datasets consisting of face identity changes are\nnot applicable for detecting facial animation videos. Further,\nwe observe that the SOTA liveness detector in public can-\nnot defend the animation data with specific actions as they\nclaimed.\nIn this paper, we propose a new dataset, called DeepFake\nMNIST+, a human face animation video dataset. The Deep-\nFake MNIST+ dataset is developed as a response to the wide\nuse of the MNIST dataset, but for a different deepfake de-\ntection problem. We create this dataset to provide the basis\nfor learning and practicing how to develop, evaluate, and\nuse deep neural networks for fake face animation detection.\nThe dataset contains 10,000 face animation videos in ten\ndifferent actions, plus 10,000 real face videos to enable a\nsupervised detector training. These fake facial animation\nare of higher fidelity and able to spoof the popular liveness\ndetectors on the market (as of the time of this manuscript\nsubmission). Given the gap of different deepfake types, the\ndetectors trained on existing DeepFake datasets with face\nidentity change cannot well detect fake animations in the\nproposed dataset. We establish deepfake detection baselines\non the DeepFake MNIST+ dataset and carefully evaluate\ntheir performance in different scenarios.\nWe also present comprehensive analysis related to the\nproperties of proposed dataset. We explore the impact of\nmotion type and compression quality of generated videos.\nAs observed from Figure 5 , the actions with large movements\nwill challenge existing detectors. But by taking these difficult\nDeepFake data into account, the learned detector can enjoy\na much better generalization across other types of DeepFake\ndata. The relevant discussion and empirical studies in this\npaper therefore shed new light on both DeepFake generation\nand detection research.\n\n2. Related Works\n\n2.1. Image Animation\n\nThe interest in facial manipulation methods has rapidly\nincreased recently. One approach for manipulation is based\non 3D modeling. Zollhofer et al., [ 42 ] build a 3D morphable\nmodel for the source face, to perform realistic animation\nfor given actions. Suwajanakorn [ 32 ] attempt to model\nlip to forgery talking. The deepfake [ 5 ] introduce a DNN\nbased face-swapping method, replacing faces within two\nidentities with two encoders. Although it requires plenty of\nvideos/images of both identities to achieve better results, the\n\npromising results show the potential of manipulation with\nDNNs. The recent researches focus on identity-independent\nswapping methods. Li et al. [ 25 ] implement two encoders to\nextract attribution and identity information and embed them\nin GAN to generate high fidelity swapped results. Several\nDNN based expression manipulation and animation are also\nproposed. Thies et al. [ 33 ] consider facial reenactment as\na domain transfer problem using Pix2Pix architecture [ 22 ]\nto produce results. Siarohin et al. [ 31 ] extract the motion\ninformation of driving videos with optimal flow estimation\nto generate high-quality animation results.\n\n2.2. DeepFake and Liveness Detection\n\nAn increasing number of DeepFake detection methods are\nproposed as a response to the huge concern from society. The\napproaches could be separated into three categories. The first\ntype is trying to detect the unnatural section of the manipu-\nlated videos, such as swapping boundary [ 26 ], inconsistent\nhead angles between the face and head [ 38 ]. The second type\ndetects the synthesis signal of GAN to distinguish DeepFake\ndata. For instance, Wang et al. [ 35 ] observe GAN signatures\nusing discrete cosine transform for detecting CNN-based\nDeepFake samples. The last categories’ approaches rely\non DeepFake dataset to train the detectors [ 30 , 7 , 18 , 29 ],\nregardless of the inconsistent or signals.\nOn the other hand, the recent liveness detector mainly\ndefending the psychical level attacks called Replay Attacks.\nFor instance, attackers could build a 3D mask of victims\nthrough a 3D printer or print out victims’ face in the paper\nand wear it. To defense against such attacks, many detection\nmethods have been proposed. Some approaches try to detect\nthe differences between real faces and forgery faces. De\net.al., [ 14 ] estimate the invariant of facial points for detec-\ntion. Komulainen [ 24 ] believe detecting faces’ dynamic\nmuscle change can distinguish the spoofing. Wang et al.,\n[ 36 ] detect the blood flow change under the skin to separate\nfacial mask and real face. The recent payment or identity ver-\nification solutions with smartpohone usually combine pose\nverification with liveness detection, such as Alipay. They\nusually require users to do some specific action, such as\nblinking or yawing, to improve the performance. However,\nour experiments show that the facial animation data with\nspecific actions could spoof the SOTA liveness detectors in\nthe market.\n\n2.3. DeepFake Datasets\n\nA few DeepFake datasets have been proposed recently\nas a response to the increasing concerns on DeepFake tech-\nniques as they can generate realistic results to spoof people.\nHowever, most of the datasets are generated using identity\nswapping algorithms, but only a few works for facial anima-\ntion. Table 1 shows the details of these DeepFake datasets.\nCeleb-DF [ 27 ] : The Celeb-DF is a large face swap\n\ndataset. It selects 590 real videos from Youtube, which\nall about the talking videos of celebrities. The dataset con-\ntains 5,639 synthesis videos using a high-resolution face\ngenerator.\n\nDFDC [ 16 ] : The Facebook DeepFake detection chal-\nlenge dataset is one of the largest face-swapping video\ndataset recently. It contains 104,500 face swap videos based\non 48,190 source videos shot with 3,426 paid actors in dif-\nferent locations and light conditions.\n\nFF++ [ 30 ] : The FaceForensics++ dataset is one of the\npopular DeepFake datasets. The autoher use multiple Deep-\nFake methods to generate the dataset. The dataset to provide\nthe data generated by expression manipulation, the related\ntechniques with facial animation. Two manipulation meth-\nods, Face2Face [ 34 ] and NeuralTextures [ 33 ], are imple-\nmented to generated DeepFake videos, while the two face-\nswapping methods [ 6 , 5 ] are also included. The dataset uses\n1,000 real videos from Youtube to generate 1,000 DeepFake\nvideos for each method.\n\nthe proposed dataset is a contemporaneous work with\nForgerynet [ 21 ]. Our proposed dataset has several differ-\nences comparing with it: i) the proposed dataset includes\nanimation data under ten specific categories (e.g., head move-\nment and emotion changes), rather than applying the un-\nknown action from a random video; ii) we boost the quality\nand challenge of the proposed dataset by filtering the gen-\neration with liveness detection; and iii) we present a com-\nprehensive analysis about the proposed dataset, including\naction categories and video quality.\n\nAccording to Table 1 , the only dataset involving the face\nanimation is FF++, which is a small dataset and is hard to\ncover the challenging deepfake data in the real world. We\nargue that the face animations in a deepfake dataset should\nbe diverse enough and are better to cover the animation\ncategories in the prospective downstream tasks, instead of\nthe just causal talking in FF++. For example, the liveness\ndetectors often require specific actions or expressions of the\nface as the input. For these reasons, it engages us to propose\na large-scale and action-specific facial animation dataset.\n\n#real\n#fake\ntype(s) of\ngeneration\naction\nspecific\n\nUADFV [ 38 ]\n49\n49\nface swap\nNo\n\nCeleb-DF [ 27 ]\n590\n5,639\nface swap\nNo\n\nDFDC [ 16 ]\n48,190\n104,500\nface swap\nNo\n\nFF++ [ 30 ]\n1000\n4000\nanimation & swap\nNo\n\nForgeryNet [ 21 ]\n91,630\n121,617\nanimation & swap\nNo\n\nDeepFake\nMNIST+\n10,000\n10,000\nimage animation\nYes\n\nTable 1: Basic information of existing DeepFake datasets.\n\n[IMAGE]\n\nFigure 1: Facial animation video samples for different ac-\ntions.\n3. DeepFake MNIST+\n\nThe major contribution of this paper is our proposed hu-\nman face animation video dataset, called DeepFake MNIST+.\nIt includes 10,000 face animation videos performing ten dif-\nferent actions and 10,000 real human face videos selected\nfrom other datasets. Besides, all these animation videos can\nspoof the liveness detection solution in the market. Such\nthat the videos are still challenging recent public detectors.\nIt is the first large-scale dataset for face animation videos\nof variant actions to the best of our knowledge. We believe\nsuch a dataset allows us to train advanced detection mod-\nels to distinguish the face animation videos for preventing\nspoofing.\n\n3.1. Generation Model and Data Preparation\n\nWe select Siarohin’s framework [ 31 ] to generate face\nanimation videos. This SOTA animation framework taking\nas input a source identity face image and a driving video\nshot by another actor. The model performs local affine trans-\n\nformations using first-order Taylor expansion to estimate the\nmotion of the driving video. Then applying the motion fea-\ntures to the generator to provide high-quality face animation\nvideos. As a result, it generates a video that animates the\nmotion of driving video while keeping the identities of the\nsource image. For more detail, please review the original\npaper. The major advantage of this framework is that the\nmodel is not identity-dependent. We can generate different\nvideos for variant identities with arbitrary driving in the one\ntrained model, while it only requires one image as the source\nimage.\nFor the source identity images, we select the frames from\nvideo in VoxCeleb1 dataset [ 28 ]. VoxCeleb1 is a large-\nscale audio-visual dataset of human speech. It includes 1251\nunique celebrities in 22,496 talking videos. All videos are\nface-cropped and have size 256x256 resolution. During\nour experiments, using the front face source images could\nachieve better generation quality. Therefore, we select the\nface frames mostly facing the front from the VoxCeleb1\ndataset as our source images for generation.\nThe DeepFake MNIST+ dataset contains forgery videos\nin 10 actions. It includes: Blink, Open mouth, Yaw, Nod,\nRight slope head, Left slope head, Look up and smile, sur-\nprise and embarrassment . The driving videos of embarrass-\nment are collected from ADFES dataset [ 19 ]. The dataset\ncontains variant emotional expression videos (anger, disgust,\nfear, joy, sadness, surprise, contempt, pride and are shot by\n22 actors. We select five actors’ embarrass videos from the\ndataset as our driving video. The videos of the remaining\nactions are shot by one volunteer. The action videos are\ncaptured using a front camera of the iPhone 11 Pro, and\neach action has been executed by the volunteer for 5 times.\nAll the driving videos are face cropped by using MTCNN\nmodules [ 40 ] and resized into 256x256 resolution to align\nthe format of the VoxCeleb1 dataset.\n\n3.2. Generating High-Quality Facial Animations\n\nWe adopt two public liveness detection APIs to select the\nchallenging samples that cannot be accurately recognized\nby the detector. The first one was provided by Tianyan-\nData [ 9 ]. TianyanData’s API supports liveness detection\nwith a specific action, including blinking, yawing, nodding,\nand opening mouth. In order to pass the detection, the input\nface video has to perform a particular action while passing\nthe spoofing test. The second one comes from Baidu [ 2 ].\nTheir detector supports universal liveness detection regard-\nless of actions. Both of these two companies claim their\ndetector can achieve 99% accuracy for detecting spoofing.\nWe generate many animation videos for all actions and\nthen pass the data to the liveness detection APIs to pick out\nthe samples that are challenging for the liveness detector.\nAs a result, the DeepFake MNIST+ dataset contains 10,000\nface animation videos in 10 specific actions and 10,000\n\nreal face videos collected from VoxCeleb1. Each action\nincludes 1,000 videos, and all of them can spoof the APIs.\nFor blinking, yawing, nodding, and opening mouth, we use\nboth TianyanData and Baidu APIs to filter videos. For the\nremaining actions, we use Baidu’s API to collect spoofed\ndata. The following graph presents each action’s spoof rate\nby passing the animation videos to the two APIs.\nThe actions of blinking, yawing, nodding and opening\nmouth have lower average spoofing rates than others, such\nthe situation could be caused by two APIs filter the videos of\nthose actions. In addition, the TianyanData’s API requires\nfurther action detection, which reduces the chance to attack.\nOn the other hand, the actions that require large-angle head\nmovement, e.g., yawing and nodding, the success spoofing\nrates are much lower than the other actions that don’t need\na significant motion change. One reasonable explanation\ncould be that a single source image of the frontal face cannot\nprovide sufficient detail of all head information, e.g., profile\nface, leading to lower head movement quality. The videos\nof simile have the highest spoofing rate, which has achieved\n61%. It might because the smile action doesn’t lead to\nsignificant head change, making it hard to detect the spoofing\ndetails. While the yaw videos are more likely to be detected,\nthat has a 23% successful rate only.\n\nDeepFake Mnist+\ndeepfake\nNeuralTextures\nFace2Face\n\noriginal accuracy\n96.58%\n99.7%\n99.2%\n98.9%\n\naccuracy on DM+\n-\n43.7%\n63.6%\n67.5%\n\nfine-tuning\n95.3%\n98.46%\n98.1%\n98.49%\n\nTable 2: The performance of Resnet50 models trained with\nexisted datasets from FF++ [ 30 ] to detect our proposed\ndataset. And the performance of DeepFake Mnist+ trained\nmodel, fine-tuning with FF++ data, to detect all these datsets.\n\nIn addition, we explore the transferability of the detec-\ntor trained with existing datasets. We train Resnet50 [ 20 ]\nmodels with the data of deepfake [ 5 ], Face2Face [ 34 ] and\nNeuralTextures [ 33 ] provided by FF++ [ 30 ] and present the\nresult for detecting DeepFake Mnist+. The first one is the\nface-swapping data. The last two are the expression manip-\nulation dataset. The Table 2 presents the result. All these\nthree models are trained with raw quality and achieve nearly\n100% accuracy in their own dataset. The result shows that\nthe face-swapping detector fails to distinguish our proposed\ndataset. It seems better in manipulation datasets but still\nhas a huge gap compared with the performance in their own\ndatasets. Furthermore, we fine-tune the DeepFake Mnist+\ntrained model with the three FF++ datasets. The result in-\ndicates that the model can gain the power to detect both\nanimation data and other Deepfake data by using our pro-\nposed dataset with other data sets.\nBased on the these results, we believe the current detec-\ntors still cannot defend against such attacks. These obser-\n\nvations engage us to proposed a face animation dataset to\nimprove the detectors and achieve better security.\n\n[IMAGE]\n\nFigure 2: The spoofing successful rates for different actions.\nThe first four actions’ videos are detected by TianyanData [ 9 ]\nand Baidu [ 2 ] API, while the later six actions’ videos are\ndetected by Baidu API.\n\n4. Benchmark Systems\n\nWe proposed a simple detection pipeline to detect the\nforgery videos in our dataset and distinguish them from those\nreal videos. The forgery detection can be formulated as a\nbinary classification task. We take the 10,000 real videos\nin the dataset as positive, while those forgery videos as\nnegative.\nIn practice, the videos are usually suffering compres-\nsion before uploading to the Internet, or the video might be\nshot by poor camera equipment, which suffering low video\nquality. Different compression rates are considered in our\nexperiment to simulate the realistic detecting setting under\ndifferent video qualities. We select two different compres-\nsion rates - C23 and C40 under H.264 codec to compress\nthe videos. To be mentioned that, a higher compression rate\nindicates worse video quality.\nWe exploited multiple models to accomplish the Deep-\nFake detection task in our experiment:\nMesoInception-4 : MesoInception [ 10 ] is a CNN model\nconsisting of two inception modules inspired by Inception-\nNet [ 10 ]. The model uses mean squared error between true\nand predicted labels rather than the ordinary cross-entropy\nloss. Following the training procedure in [ 30 ], we extracted\nthe frames as the original size which is 256x256 resolution.\nXceptionNet : The XceptionNet [ 13 ] is a traditional CNN\nmodel based on separable convolutions with residual con-\nnections. The model has shown high accuracy when detect-\ning deepfake [ 5 ] videos and has been the baseline model\nintroduced by Rossler et al. [ 30 ]. We used a pre-trained\nmodel on the ImageNet [ 15 ] dataset in this experiment. The\nCNN layers are frozen, and we only update the weights of\nnewly inserted full connected layers for the prediction. Same\nwith MesoInception, we also used the 256x256 resolution of\nframes as the input.\n\nResnet : The Residual Neural network (Resnet) [ 20 ] is\none of the most popular neural networks. It utilizes skip con-\nnections or shortcuts to jump over some network layers, such\nthat the networks are easier to be optimized even with the\nincreasing depth. The Resnet networks are also pre-trained\nwith ImageNet [ 15 ]. Three versions of Resnet - Resnet50,\nResnet101, and Resent152 are included in the experiment\nto explore the performance change under different network\ndepths. For all versions of Resnet, we capture and resize the\nimage frames into 224x244 resolution.\nAll CNN models are trained with Adam optimizer with\nan initial learning rate of 0 . 0002 , and we set \\ beta _1 = 0.999 and\n\\ beta _2 = 0.9. The learning rate decreases under the poly-decay\nschedule. The total number of training epoches for each\nmodel is set as 50, and the batch size is 64. We pick 70%\nof videos from the DeepFake MNIST+ as the training data,\nthat is, 700 videos from each of the ten action categories\nand 7000 real videos. 15% videos are the validation data.\nThe remaining 15% will be the test data. We select the best\nversions of models based on the accuracy on the validation\nset.\n5. Evaluation\n\nThis section presents different models’ performance\nchanges for detecting our proposed face animation video\ndataset under different situations. We present different mod-\nels’ accuracy for classifying test video data frames as real or\nfake ones to show the performance.\n5.1. Overall Detection Performance\n\nTable 3 compared the accuracy of different models un-\nder three different video compression levels (raw, light and\nheavy compression). The result indicates that the Resnet\nmodels have the best performance among all models. They\nachieve a 96.3% average accuracy on the raw video dataset,\nwhich is much higher than those of the other two CNN mod-\nels. The XceptionNet, that also has a deep architecture, ap-\nproaches a 92.38% accuracy for detecting forgery videos. In\naddition, the MesoNet shows the poorest performance with\nonly a 60.39% accuracy for detecting the raw videos. The\nincreasing of the network depth does not constantly boost\nthe performance. The accuracy of three Resnet variants are\nvery similar. It is hard to say, the deeper architectures, e.g.,\nResnet152, show improvement compared to other smaller\nmodels.\nThe video quality is also an important factor affecting the\nperformance. Table 3 shows that worse video quality (higher\ncompression rate) could lead to a performance downgrade.\nCompared to the raw dataset, the average accuracy of Resnet\nnetworks decreases around 2% under the light compression\ncondition. The situation is worse when the videos are un-\nder heavy compression. 91.06% of compressed videos are\nclassified correctly with Resnet networks on average, which\nhas a 5% gap from that on the raw dataset. The XceptionNet\n\nTable 3: The accuracy of different classifier models in testing set under different compression rate. The light compression\ncorresponding to the c23 compression rate, while the heavy compression corresponding to the c40 compression rate. We\nselect the best version of models based on the validation accuracy during the training process.\n\nResnet50\nResnet101\nResnet152\nXceptionNet\nMesoNet\n\nraw\n96.58%\n96.64%\n96.18%\n92.38%\n60.39%\nlight compression\n94.32%\n94.87%\n94.90%\n85.52%\n58.58%\nheavy compression\n91.49%\n90.44%\n91.27%\n83.143%\n57.90%\n\nRaw - > LC\nRaw - > HC\nLC - > Raw\nLC - > HC\nHC - > Raw\nHC - > LC\n\nResnet50\n85.52%\n71.3%\n95.16%\n82.98%\n59%\n61.02%\nResnet152\n79.23%\n68.12%\n82.33%\n73.60%\n76.72%\n76.69%\nXception\n79.17%\n71.83%\n87.89%\n68.99%\n67.38%\n62.9%\nMesoNet\n51.37%\n56.92%\n57.01%\n49.53%\n55.65%\n58.64%\n\nTable 4: The models’ performances trained by one specific compression rate and detecting the videos from the datasets of the\nother two compression rates.\n\nis more sensitive to compression rate than other networks,\nwhose accuracy drops to 85.52% significantly when applying\nlight compression and 83.143% for heavy compression.\nThe decreasing correction rate could be caused by the\nloss of detail in low video quality. The compression process\nleads to blur frames, hiding the forgery information so that\nthe detector might not be able to capture such information\nfor detecting the forgery areas.\n\n5.2. Analysing the Impact of Video Quality\n\nWe further explore how the video quality could affect the\nperformance. The Table 4 shows the models’ generalization\nfor the videos of different compression rates. It presents\nthe accuracy of models trained with one quality (e.g., Raw)\nand predicts the data with the other two qualities(e.g., light\nand heavy compressed). The results indicate that the models\ncannot adapt well to datasets with other qualities, especially\nbetween raw and heavy compression datasets. The raw video\nmodels only achieve 70% accuracies on average for heavy\ncompression videos and 67% conversely. It might show\nthe heavy compression could change the data distribution\nleading to a dramatic accuracy drop. Also, light compression\nmodels have better generalizations than others, indicating the\nmodels could learn animation and compression information\nsimultaneously.\nOne way to overcome the impact of different video quality\nis to train the network with the videos in all qualities. We\ntrain the Resnet50 and Resnet152 with the mixed video\nquality dataset for the experiment. More specifically, we\nselect the video quality randomly for each sample to train\nthe models and present their performances in testing sets\nunder three qualities respectively. The Table 5 shows the\nresult. With training in mixed quality videos, the models can\nadapt to different video qualities. However, it still has a slight\n\nResnet50\nResnet152\nXceptionNet\nMesoNet\n\nRaw\n93.57%\n95.78%\n90.82%\n59.47%\nLC\n90.69%\n92.11%\n83.28%\n56.94%\nHC\n85.56%\n88.32%\n82.43%\n55.38%\n\nTable 5: The performances of models trained with mixed\nvideo quality dataset in different testing sets.\n\naccuracy decrease, especially for heavy compression videos.\nThis change supports our previous observation, which could\nhave a large difference in distribution between raw and heavy\ncompressed videos. The result also suggests that it might\nrequire a large model to learn the mixed video quality dataset.\nResnet152, a deeper network, has smaller gaps with the\nmodels trained with a single-quality video set, which has\n3% improvement for all qualities compare to the smaller\nResnet50 network.\n\n5.3. Evaluation of the Training Corpus Size\n\nWe evaluate how the training corpus size could affect the\ndetection performance. We select 10000, 3000, 1000, 500,\n100, and 10 animation videos and the corresponding number\nof real videos to train the Resnet50 model. The animation\nvideos for each action are selected equally.\nThe chart on the left of Figure 3 presents the importance\nof training corpus size. It could only lead to a small down-\ngrade in the raw dataset when keeping 10% of the data, but\nmore impact when the videos are compressed. It could in-\ndicate that more low-quality videos are required to train the\nmodels. Besides, correction decreases dramatically if we\nuse 1% of data for training in all quality. Simultaneously,\nthe models tend to random guessing when we only have ten\nanimation videos for training.\nIn addition, Figure 4 shows performances under different\n\nproportions of animation and real data for training. More\nspecifically, we reduce the number of one class’s videos\n(either real or animation) to reach the expected proportions\nof videos for training. For example, ”1:2 more real videos”\nmeans we remove half of the animation videos for training.\nSimilar to decrease the total training corpus size, either re-\nducing the real or animation videos will lead to an accuracy\ndecrease. Also, preserving animation videos for training is\nmore critical than introducing more real videos. When we\nuse 10% of animation video for training, the performance\nonly achieves 85%; a 10% decrease compares to the full\ndataset. On the contrary, keeping 10% of real videos with\nfull animation videos leads to a smaller 5% downgrade.\nThe chart on the right of Figure 3 presents how the\ntraining corpus size could affect the performance of mod-\nels trained with the mixed quality dataset. We trained the\nResnet152 models with 3000, 1000, 500, 100, and 10 ani-\nmation videos and the corresponding number of real videos\nunder mixed video quality. Similar to the models trained with\nsingle quality datasets, a smaller corpus size also reduces\ncorrections for the mixed quality situation. We also notice\nthat the performance will slightly lower for large training cor-\npus size than single-quality videos trained models. However,\nwhen the corpus size becomes smaller, the accuracy tends to\nbe higher than the model trained with single-quality videos,\nespecially for light compression videos, which suffering the\nmost impact relate to decreased corpus size. One reason\ncould be that the mixed video quality training strategy is sim-\nilar to data augmentation, which increases the data diversity\nfor small training corpus size to improve performance.\n\n[IMAGE]\n\nFigure 3: The performance changes under different training\ncorpus sizes using single quality or mixed quality video\ndataset.\n\n5.4. Evaluating the Impact of Type of Actions\n\nThe type of actions could affect the detection perfor-\nmance. We train the models with videos of one single action\nand evaluate whether the models could adapt to other unseen\nanimation videos. For each action, we select all animation\nvideos of that action and the corresponding number of real\nvideos (1000 videos for each label) to train the Resnet50\nmodels and test the performance with the whole animation\ntesting video set. We select the raw quality videos for the\nexperiment, and Figure 5 shows the result. With the train-\ning of single-action, it is not doubted that models cannot\n\n[IMAGE]\n\nFigure 4: The detection accuracy of Resnet50 models trained\nwith raw videos under different proportions of real and ani-\nmation videos.\n\nkeep similar performance to the one trained with full actions\nraw quality videos with 1000 corpus size. The accuracy\ndrops to 74.3% on average from 93.4%. The videos of nod-\nding and surprising actions provide a better generalization\nto adapt unseen actions, which achieve 80.46% and 79.98%\nrespectively. Right and left slope videos also introduce rela-\ntively high performance than remaining actions, which reach\n77.02% and 75.89% respectively. On the other hand, the\nmodels trained with smile and blink videos have poor correc-\ntion rates, which drop to 67.36% and 69.08%. In summary,\nusing large movement action videos to train classifiers could\nlead to better performance for detecting new actions’ videos.\nThe actions that only include small changes, e.g., smiling\nand blinking, cannot provide sufficient information for the\nnetworks to adapt to unseen videos.\nWe also compare the full dataset trained models’ per-\nformances for detecting different actions under three video\nqualities, and Figure 6 shows the result. We can notice that\nsome actions’ videos are relatively hard to detect in all video\nqualities. The left slope videos are the most difficult ones,\nwhich the accuracy is 92.3% under raw videos and drop to\n88.24% under heavy compression videos. In addition, the de-\ntection of embarrassment videos might require higher video\nquality. Its performance decreases to 87.5%, the lowest one\nunder heavy compression and a huge 7% gap compare to the\none in raw quality. On the other hand, some actions, e.g.,\nsmiling, blinking, are much easier to be detected with the\nnetworks. They have achieved 100% correctness for raw\nvideos, decrease to 97% on average if the videos are heavily\ncompressed, which still keep in a higher level than other\nactions.\nComparing with Figure 5 , we observe that some hard-\nto-detect actions, e.g., right and left slope, could provide\nmore generalization if we only use those actions’ videos for\ntraining. On the contrary, the models trained with the videos\nof easy-to-detect actions, e.g., smiling and blinking, showing\npoorer performances for adapting unseen actions.\n\n5.5. Visualizing the Attention Parts of Models\n\nWe analyze what the classifier learned for distinguishing\nthe animation and real videos. More specifically, we try\n\n[IMAGE]\n\nFigure 5: The detection accuracy for the Resnet50 models\ntrained with one specific actions videos. Each label indicates\nthe model of the whole testing data performance, trained\nwith the videos of that specific action and 1,000 real videos.\n\n[IMAGE]\n\nFigure 6: The detection accuracy for each action under differ-\nent video quality. The models are trained with full training\ndataset. The original class indicate the selected real videos.\n\nto visualize the network pay attention to which part of the\nvideo frames for detection. We use Class Activation Map\n(CAM) [ 41 ] to achieve visualization. The CAM is a tech-\nnique to visualize what the classifier is looking at. CAM\nrelies on the output from the models’ global average pool-\ning(GAP) layer, which right after the last convolution layer.\nThe GAP layer could keep the spatial information of the\nconvolution layer. By multiplying the weight of the Softmax\nlayer of one specific class with GAP output, we can visualize\nthe models’ attention regions for classifying the given input\nimages as that specific class. Our baseline models are binary\nclassifiers to separate frames from either real and animation\nvideos. In this case, the CAM results present semantic infor-\nmation about which parts of input image are critical regions\nfor the models to decide whether it is from the animation\nvideos.\n\nIn our experiment, we visualize the CAM results of the\nResnet50 model trained with raw quality videos. To be\nnoticed that, the GAP layer has been added to the model\nin the original design, so we don’t require to do the further\nmodification. The Figure 7 demonstrates some CAM results\nof selected video frames of different actions. We adapt the\nresults to the original images to highlight the attention parts.\nThe red color region indicates the important region, while the\nmodel pays less attention to the blue color areas. The results\nindicate that the model could learn semantic information to\ndetect the animation videos. The model relies on the forgery\nregions to make the decision. For the opening mouth video\nframes, the model focuses on the detailed information of\nmouth. Similarly, the head and neck regions of the frames\ncould be significant for detecting up videos. And the network\npays attention to the profile face for head movement action’s\nvideo frame, like yawing and sloping.\n\n[IMAGE]\n\nFigure 7: The Class Activation Map (CAM) results of the\nResnet50 model trained with raw videos.\n\n6. Discussion and Conclusion\n\nWe present a new large-scale, action-specified facial an-\nimation video dataset - Deepfake Mnist+, and evaluate the\ndataset’s detecting performance with the proposed baseline\ndetection method in different situations. We mainly explore\nthe impact of the compression and actions on classification\naccuracy. It indicates the low-quality videos could signifi-\ncantly affect the performance and large movement actions\ncould provide further generalization for unseen data. We\nexpect that our proposed dataset could improve the detec-\ntion performance of facial animation videos and increase the\nrobustness and security of the recent liveness detectors as a\nresponse to the concern about pervasive animation videos\nonline. As future work, we will explore other advanced fa-\ncial animation methods and enlarge our datasets with more\nactions shot by more actors.\n\nReferences\n\n[1] Avatarify.\nhttps://avatarify.ai/ .\n(Accessed on\n03/17/2021). 2\n[2] Baidu ai.\nhttps://ai.baidu.com/tech/face/\nfaceliveness . (Accessed on 03/17/2021). 4 , 5\n[3] Fakeapp 2.2.0 - download for pc free.\nhttps://www.\nmalavida.com/en/soft/fakeapp/ .\n(Accessed on\n03/17/2021). 1\n[4] Github - deepfakes/faceswap:\nDeepfakes software for\nall. https://github.com/deepfakes/faceswap .\n(Accessed on 03/17/2021). 1\n[5] Github - dfaker/df: Larger resolution face masked, weirdly\nwarped, deepfake,.\nhttps://github.com/dfaker/\ndf . (Accessed on 03/17/2021). 1 , 2 , 3 , 4 , 5\n[6] Github - iperov/deepfacelab: Deepfacelab is the leading soft-\nware for creating deepfakes.\nhttps://github.com/\niperov/DeepFaceLab . (Accessed on 03/17/2021). 1 ,\n3\n[7] Github - selimsef/dfdc deepfake challenge: A prize win-\nning solution for dfdc challenge. https://github.com/\nselimsef/dfdc_deepfake_challenge . (Accessed\non 03/17/2021). 1 , 2\n[8] Github - shaoanlu/faceswap-gan:\nA denoising autoen-\ncoder + adversarial losses and attention mechanisms for\nface swapping.\nhttps://github.com/shaoanlu/\nfaceswap-GAN . (Accessed on 03/17/2021). 1\n[9] Tianyandata. https://www.tianyandata.cn/ . (Ac-\ncessed on 03/17/2021). 4 , 5\n[10] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao\nEchizen. Mesonet: a compact facial video forgery detection\nnetwork. In 2018 IEEE International Workshop on Informa-\ntion Forensics and Security (WIFS) , pages 1–7. IEEE, 2018.\n5\n[11] Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser\nSheikh. Recycle-gan: Unsupervised video retargeting. In\nProceedings of the European conference on computer vision\n(ECCV) , pages 119–135, 2018. 1\n[12] Egor Burkov, Igor Pasechnik, Artur Grigorev, and Victor Lem-\npitsky. Neural head reenactment with latent pose descriptors.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 13786–13795, 2020. 1\n[13] Fran c¸ ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n1251–1258, 2017. 5\n[14] Maria De Marsico, Michele Nappi, Daniel Riccio, and Jean-\nLuc Dugelay. Moving face spoofing detection via 3d projec-\ntive invariants. In 2012 5th IAPR International Conference\non Biometrics (ICB) , pages 73–78. IEEE, 2012. 2\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition , pages 248–255. Ieee, 2009. 5\n[16] Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram,\nand Cristian Canton Ferrer. The deepfake detection challenge\n(dfdc) dataset. arXiv preprint arXiv:2006.07397 , 1(2), 2020.\n1 , 3 , 12\n\n[17] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio.\nGenerative adversarial networks.\narXiv\npreprint arXiv:1406.2661 , 2014. 1\n[18] David G ¨ uera and Edward J Delp. Deepfake video detection\nusing recurrent neural networks. In 2018 15th IEEE Inter-\nnational Conference on Advanced Video and Signal Based\nSurveillance (AVSS) , pages 1–6. IEEE, 2018. 2\n[19] ST Hawk, J Van der Schalk, and AH Fischer. Moving faces,\nlooking places: The amsterdam dynamic facial expressions\nset (adfes). In 12th european conference on facial expressions,\ngeneva, switzerland , volume 4, 2008. 4 , 11\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 770–778, 2016. 4 , 5\n[21] Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun\nYin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu.\nForgerynet: A versatile benchmark for comprehensive forgery\nanalysis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 4360–4369,\n2021. 3\n[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\nImage-to-image translation with conditional adversarial net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 1125–1134, 2017. 2\n[23] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and\nChen Change Loy. Deeperforensics-1.0: A large-scale dataset\nfor real-world face forgery detection.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 2889–2898, 2020. 12\n[24] Jukka Komulainen, Abdenour Hadid, and Matti Pietik ¨ ainen.\nFace spoofing detection using dynamic texture. In Asian\nConference on Computer Vision , pages 146–157. Springer,\n2012. 2\n[25] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang\nWen. Advancing high fidelity identity swapping for forgery\ndetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 5074–5083,\n2020. 1 , 2\n[26] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen,\nFang Wen, and Baining Guo. Face x-ray for more general\nface forgery detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5001–5010, 2020. 1 , 2\n[27] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei\nLyu. Celeb-df: A large-scale challenging dataset for deepfake\nforensics. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 3207–3216,\n2020. 1 , 2 , 3 , 12\n[28] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew\nZisserman. Voxceleb: Large-scale speaker verification in the\nwild. Computer Speech & Language , 60:101027, 2020. 4 , 11\n[29] Huy H Nguyen, Junichi Yamagishi, and Isao Echizen.\nCapsule-forensics: Using capsule networks to detect forged\nimages and videos. In ICASSP 2019-2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 2307–2311. IEEE, 2019. 2\n\n[30] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-\ntian Riess, Justus Thies, and Matthias Nießner. Faceforen-\nsics++: Learning to detect manipulated facial images. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 1–11, 2019. 1 , 2 , 3 , 4 , 5\n[31] Aliaksandr Siarohin, St ´ ephane Lathuili ` ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. arXiv preprint arXiv:2003.00196 , 2020. 1 ,\n2 , 3 , 11\n[32] Supasorn\nSuwajanakorn,\nSteven\nM\nSeitz,\nand\nIra\nKemelmacher-Shlizerman. Synthesizing obama: learning\nlip sync from audio. ACM Transactions on Graphics (ToG) ,\n36(4):1–13, 2017. 1 , 2\n[33] Justus Thies, Michael Zollh ¨ ofer, and Matthias Nießner. De-\nferred neural rendering: Image synthesis using neural textures.\nACM Transactions on Graphics (TOG) , 38(4):1–12, 2019. 1 ,\n\n2 , 3 , 4\n[34] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian\nTheobalt, and Matthias Nießner. Face2face: Real-time face\ncapture and reenactment of rgb videos. In Proceedings of the\nIEEE conference on computer vision and pattern recognition ,\npages 2387–2395, 2016. 1 , 3 , 4\n[35] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\nOwens, and Alexei A Efros.\nCnn-generated images are\nsurprisingly easy to spot... for now.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 8695–8704, 2020. 2\n[36] Shun-Yi Wang, Shih-Hung Yang, Yon-Ping Chen, and Jyun-\nWe Huang. Face liveness detection based on skin blood flow\nanalysis. symmetry , 9(12):305, 2017. 2\n[37] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video\nsynthesis. arXiv preprint arXiv:1808.06601 , 2018. 1\n[38] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes\nusing inconsistent head poses. In ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 8261–8265. IEEE, 2019. 1 , 2 , 3\n[39] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and\nVictor Lempitsky. Few-shot adversarial learning of realistic\nneural talking head models. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 9459–\n9468, 2019. 1\n[40] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.\nJoint face detection and alignment using multitask cascaded\nconvolutional networks. IEEE Signal Processing Letters ,\n23(10):1499–1503, 2016. 4 , 11\n[41] B. Zhou, A. Khosla, Lapedriza. A., A. Oliva, and A. Tor-\nralba. Learning Deep Features for Discriminative Localiza-\ntion. CVPR , 2016. 8\n[42] Michael Zollh ¨ ofer, Justus Thies, Pablo Garrido, Derek\nBradley, Thabo Beeler, Patrick P ´ erez, Marc Stamminger,\nMatthias Nießner, and Christian Theobalt. State of the art on\nmonocular 3d face reconstruction, tracking, and applications.\nIn Computer Graphics Forum , volume 37, pages 523–550.\nWiley Online Library, 2018. 2\n\nA. Data Generation And Collection Pipeline\n\nThe Figure 8 shows the pipeline to generate and collect\nour proposed DeepFake MNIST+ dataset. First, we collect\nthe real videos from the VoxCeleb1 [ 28 ], extract the frames\nfrom these real videos as the source identity images. Then\nwe shot driving videos with ten actions through the volun-\nteers. In order to match the format of VoxCeleb1 videos,\nwhich are face-cropped and have the size of 256x256 resolu-\ntion. We made a face-cropped version of driving videos with\nMTCNN modules [ 40 ] and also resize them into 256x256\nresolution. We use Siarohin’s framework [ 31 ] for animation\nvideo generation to produce face animation videos. It is a\nSOTA animation framework such that it even could capture\nthe detail of eyeball moving. A single source image and\na driving video were passed to the generator each time to\nproduce single animation videos with a specific action. The\ngenerated videos were filtered with Liveness detector APIs\nto collect the challenging videos. Finally, 10,000 passed ani-\nmation videos with ten actions (1000 videos for each action)\nand selected 10,000 real videos from VoxCeleb1 from our\nproposed DeepFake MNIST+ dataset.\n\nB. More Examples of Data\n\nB.1. Compressed Images\n\nWe demonstrate some raw and compressed (in both C23\nand C40 compression rate under H.264 codec) video frames\nin Figure 9 . The higher rate means heavier compression.\nAs we can see, the c23 compression rate only leads to a\nminor impact on visual video quality. However, the frames\nsuffer significant detail loss and blur effect under the c40\ncompression rate.\n\nB.2. Driving Video samples\n\nIn this section, we present some driving video samples\nof different actions for animating the source images in Fig-\nure 10 . The driving videos of embarrassment are picked\nfrom ADFES dataset [ 19 ].\n\n[IMAGE]\n\nFigure 8: The pipeline to generate and collect our proposed DeepFake MNIST+ dataset.\n\nC. More experiments\n\nDFDC[ 16 ]\nDF-1.0[ 23 ]\nCeleb-DF[ 27 ]\nwithout finetune\n61.2%\n60.7%\n57.2%\nfinetune\n95.6%\n96.1%\n95.1%\n\nTable 6: Accuracy of detecting DeepFake Mnist+ using the\nmodels trained with previous datasets. Fine-tuning means\nfine-tuned with proposed dataset.\n\n[IMAGE]\n\nFigure 9: The samples of raw and corresponding compressed (both c23 and c40) video frames.\n\n[IMAGE]\n\nFigure 10: The driving video samples for animating the source images."
  },
  "quality_metrics": {
    "pages_with_text": 14,
    "pages_empty": 0
  },
  "processing_metadata": {
    "PDF Type": "text_based",
    "Pages": 14,
    "File Size (MB)": "14.5",
    "Processing Time (s)": "0.00",
    "Extraction Method": "pymupdf_structured",
    "Confidence": "1.00",
    "Characters Extracted": 48813,
    "Memory Peak (MB)": "0.0"
  }
}